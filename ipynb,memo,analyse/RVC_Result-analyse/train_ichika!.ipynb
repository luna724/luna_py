{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part.1\n",
    "    # Training  - First Training -\n",
    "        Model Name: ichikaa\n",
    "        Ignore cache: False\n",
    "        Dataset: Not Changed\n",
    "        Recursive: False  |  Multiple Speakers: False\n",
    "        Speaker ID: 0 \n",
    "        Model Version: v2  |  Target Sampling Rate: 48k\n",
    "        f0 Model: Yes  |  Using Phone Embedder: hubert-base-japanese\n",
    "        Embedding Channels: 768\n",
    "        Embedding Output Layer: 12\n",
    "        Number of CPU Processes: 4\n",
    "        Normalize audio volume when preprocess: Yes\n",
    "        Pitch Extraction algorithm: Crepe\n",
    "        Batch Size: 12  |  Epochs: 25\n",
    "        Cache Batch: True  |  FP16: False\n",
    "        Augment: False  |  Augment From Pretrain: False\n",
    "        Pretrained Generator Path (pth): Not Selected\n",
    "        Speaker Info path (npy): Not Selected\n",
    "        Pre trained generator path: f0G48k768.pth\n",
    "        Pre trained discriminator path: f0D48k768.pth\n",
    "        Train Index: Yes  |  Reduce Index Size with Kmeans: No\n",
    "        Maximum Index Size: 10000\n",
    "        Learning Rate: 0.0001  |  Learning Rate Decay (lr_decay): 0.999875\n",
    "        Warmup_Epochs: 0  |  eps: 1e-9  |  Segment_size: 11520\n",
    "        c_mel: 45  |  c_kl: 1.0  |  Seed: 1234\n",
    "        init_lr_ratio: 1  |  Sampling_rate: 45000 \n",
    "    \n",
    "    # Result (Train)\n",
    "        Total Processing Count: 24850\n",
    "        ETA: 2:23:54  2.82it / s\n",
    "        loss_Discliminator (loss_d):  1.99\n",
    "        loss_Generator (loss_g)    : 48.30\n",
    "        Last LR\n",
    "    \n",
    "    # Enviroment Info\n",
    "        Machine Type: Google Colab\n",
    "        GPU Type: NVIDIA Tesla V100  Normal Memoru (12RAM / 16VRAM)\n",
    "\n",
    "    # Result (Infer)  - Source 絶え間なく藍色 -\n",
    "        Speaker ID: 0  |  Source Audio: 絶え間なく藍色 ft.初音ミク\n",
    "        Transpose: 0  |  PitchExtraction Algrorithm: Crepe\n",
    "        Embedder Model: Hubert-base-japanese\n",
    "        Embedder Output Layer: 12\n",
    "        Auto Load Index: True\n",
    "        F0 Curve File: Not Selected\n",
    "        Result Count: 1\n",
    "\n",
    "# 結果\n",
    "| Number | Source Name | Result File | Vocal | pronunciation | Accuracy | Total | \n",
    "|---|---|---|---|---|---|---|\n",
    "| No.1 | 絶え間なく藍色 ft. 初音ミク | [Result 1  ~絶え間なく藍色 ft. 初音ミク~](./train_ichika/p1/result_taemanaku_miku.wav) | 70 | 35 | 20 | 125 |\n",
    "\n",
    "\n",
    "# 点数の詳細\n",
    "    # No.1  ~絶え間なく藍色 ft. 初音ミク~\n",
    "        Vocal Point: 声はかなり似ている部分がある。\n",
    "                     二重ボーカルもある程度変換できていた\n",
    "                     しかし、ミクの高音や、低音ができていない\n",
    "                     (70 / 100)\n",
    "        \n",
    "        Pronunciation Point: 発音は、かなりミクで変換ができていない。\n",
    "                             もしかしたら、発音の変換はしないのかもしれない。\n",
    "                             (35 / 100)\n",
    "        \n",
    "        Accuracy Point: 発音の良さを表すポイント\n",
    "                        発音は、かなりミクといったが、一部高音が混じっているところなどは、\n",
    "                        変換がうまくいかず、声がかすれていたり、活舌や、発音がよくない。\n",
    "                        (20 / 100)\n",
    "\n",
    "        Total Point: ボーカルの変換がかなり良くできている。\n",
    "                     ただし、発音や高音の変換ができないという欠点があったため、低めの点になった。\n",
    "                     (125 / 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m traintitle \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining Title: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m modelname \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mモデル名: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m ignorecache \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mIgnore Cache(0 or 1): \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     24\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDataset: (変更内容)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m recursive \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRecursive(0 or 1): \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1191\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[0;32m   1192\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[0;32m   1193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1195\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1196\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1232\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def tfgen(inf):\n",
    "    if inf == 0:\n",
    "        return \"False\"\n",
    "    elif inf == 1:\n",
    "        return \"True\"\n",
    "    else:\n",
    "        return str(inf)\n",
    "def yngen(inf):\n",
    "    if inf == 0:\n",
    "        return \"No\"\n",
    "    elif inf == 1:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return str(inf)\n",
    "a = False\n",
    "b = False\n",
    "c = False\n",
    "originalgpu = False\n",
    "resulttrain_printout = \"\"\"\\n\"\"\"\n",
    "# Result M↓ Generator\n",
    "traintitle = input(\"Training Title: \")\n",
    "modelname = input(\"モデル名: \")\n",
    "ignorecache = input(\"Ignore Cache(0 or 1): \")\n",
    "dataset = input(\"Dataset: (変更内容)\")\n",
    "recursive = input(\"Recursive(0 or 1): \")\n",
    "multispeaker = input(\"Multiple Speakers(0 or 1): \")\n",
    "ignorecache = tfgen(ignorecache)\n",
    "recursive = tfgen(recursive)\n",
    "multispeaker = tfgen(multispeaker)\n",
    "speakerid = str(input(\"Speaker ID: \"))\n",
    "modelver = input(\"Model Version(v1 or v2): \")\n",
    "targetsr = str(input(\"Target Sampling Rate: \"))\n",
    "f0model = input(\"F0 Model (0 or 1): \")\n",
    "f0model = tfgen(f0model)\n",
    "usepe = input(\"Using Phone Embedder(0(hubertjp) or 1(contentvec)): \")\n",
    "if tfgen(usepe) == \"True\":\n",
    "    usepe = \"ContentVec\"\n",
    "elif tfgen(usepe) == \"False\":\n",
    "    usepe = \"Hubert-base-Japanese\"\n",
    "else:\n",
    "    usepe = tfgen(usepe)\n",
    "embchannel = str(input(\"Embedding Channels(256 or 768): \"))\n",
    "emboutlayer = str(input(\"Embedding Output Layer(9 or 12): \"))\n",
    "numcpu = str(input(\"Number of CPU Processes: \"))\n",
    "normalizevolume = input(\"Normalize Audio Volume When Preprocess(0 or 1): \")\n",
    "normalizevolume = tfgen(normalizevolume)\n",
    "pitchalg = input(\"Pitch Extraction algorithm: \")\n",
    "bsize = str(input(\"Batch Size: \"))\n",
    "epoch = str(input(\"Epochs: \"))\n",
    "bcache = input(\"Cache Batch(0 or 1): \")\n",
    "bcache = tfgen(bcache)\n",
    "fp16 = input(\"FP16(0 or 1): \")\n",
    "fp16 = tfgen(fp16)\n",
    "augment = input(\"Augment(0 or 1): \")\n",
    "augment = tfgen(augment)\n",
    "augfpretrain = input(\"Augment From Pretrain(0 or 1): \")\n",
    "augpre = tfgen(augfpretrain)\n",
    "preg_ = input(\"Pre Trained Generator Path(pth)(No Input = Not Selected): \")\n",
    "if preg_ == \"\":\n",
    "    preg_ = \"Not Selected\"\n",
    "spinfonpy = input(\"Speaker Info Path (npy) (NoInput = Not Selected): \")\n",
    "if spinfonpy == \"\":\n",
    "    spinfonpy = \"Not Selected\"\n",
    "preg = input(\"PreTrained Generator Path: \")\n",
    "pred = input(\"PreTrained Discliminator Path: \")\n",
    "trainindex = input(\"Train Index(0 or 1): \")\n",
    "trainindex = yngen(trainindex)\n",
    "rindex = input(\"Reduce Index Size with Kmeans(0 or 1): \")\n",
    "rindex = yngen(rindex)\n",
    "maxisize = str(input(\"Max Index Size: \"))\n",
    "lr = str(input(\"Learning Rate: \"))\n",
    "lrd = str(input(\"Learning Rate Decay: \"))\n",
    "wu_epoech = str(input(\"Warmup Epochs: \"))\n",
    "eps = str(input(\"eps(Default = 1e-9): \"))\n",
    "ss = str(input(\"Segment Size: \"))\n",
    "cmel = str(input(\"c_mel: \"))\n",
    "ckl = str(input(\"c_kl: \"))\n",
    "seed = str(input(\"Seed: \"))\n",
    "initlrr = str(input(\"Init LR Ratio: \"))\n",
    "sr = str(input(\"Sampling Rate: \"))\n",
    "\n",
    "training_printout = f\"\"\"\n",
    "# Training  - {traintitle} -\n",
    "    Model Name: {modelname}\n",
    "    Ignore Cache: {ignorecache}\n",
    "    Dataset: {dataset}\n",
    "    Recursive: {recursive}  |  Multiple Speakers: {multispeaker}\n",
    "    Speaker ID: {speakerid}\n",
    "    Model Version: {modelver}  | Target Sampling Rate: {targetsr}\n",
    "    f0 Model: {f0model}  |  Using Phone Embedder: {usepe}\n",
    "    Embedding Channels: {embchannel}\n",
    "    Embedding Output Layer: {emboutlayer}\n",
    "    Number of CPU Processes: {numcpu}\n",
    "    Normalize audio volume when Preprocess: {normalizevolume}\n",
    "    Pitch Extraction Algorithm: {pitchalg}\n",
    "    Batch Size: {bsize}  |  Epochs: {epoch}\n",
    "    Cache Batch: {bcache}  |  FP16: {fp16}\n",
    "    Augment: {augment}  |  Augment From Pretrain: {augfpretrain}\n",
    "    Pretrained Generator Path (pth): {preg_}\n",
    "    Speaker Info Path (npy): {spinfonpy}\n",
    "    Pre trained Generator path: {preg}\n",
    "    Pre trained Discriminator path: {pred}\n",
    "    Train Index: {trainindex}  |  Reduce Index Size With Kmeans: {rindex}\n",
    "    Maximum Index Size: {maxisize}\n",
    "    Learning Rate: {lr}  |  Learning Rate Decay(lr_decay): {lrd}\n",
    "    Warmup_Epochs: {wu_epoech}  |  eps: {eps}  |  Segment_size: {ss}\n",
    "    c_mel: {cmel}  |  c_kl: {ckl}  |  Seed: {seed}\n",
    "    init_lr_ratio: {initlrr}  |  Sampling_rate: {sr}\n",
    "\n",
    "\"\"\"\n",
    "while a:\n",
    "    next = input(\"Are you want Result (Train) Input? (Yes or No): \")\n",
    "    if next == \"Yes\":\n",
    "        # Result (Train)\n",
    "        tpc = str(input(\"Total Processing Count: \"))\n",
    "        eta = str(input(\"ETA (e.g: 2:23:65  2.82it / s): \"))\n",
    "        lossd = str(input(\"Loss Discriminator: \"))\n",
    "        lossg = str(input(\"Loss Generator: \"))\n",
    "        lastlr = str(input(\"Last Lr: \"))\n",
    "        \n",
    "        resulttrain_printout = f\"\"\"\n",
    "        # Result (Train)\n",
    "            Total Processing Count: {tpc}\n",
    "            ETA: {eta}\n",
    "            loss_Discriminator (loss_d): {lossd}\n",
    "            loss_Generator (loss_g): {lossg}\n",
    "            Last LR: {lastlr}\n",
    "        \n",
    "        \"\"\"\n",
    "        a = True\n",
    "        break\n",
    "    elif next == \"No\":\n",
    "        print(training_printout)\n",
    "        a = True\n",
    "        break\n",
    "    else:\n",
    "        a = False\n",
    "\n",
    "while b:\n",
    "    next = input(\"Are you want Enviroment Info Input? (Yes or No): \")\n",
    "    if next == \"Yes\":\n",
    "        machinetype = input(\"Machine Type(0(Google Colab) or 1(Local)): \")\n",
    "        if tfgen(machinetype) == \"True\":\n",
    "            machinetype = \"Local PC\"\n",
    "        elif tfgen(machinetype) == \"False\":\n",
    "            machinetype = \"Google Colab\"\n",
    "        else:\n",
    "            machinetype = tfgen(machinetype)\n",
    "        if machinetype == \"Google Colab\":\n",
    "            gputype = input(\"GPU Type(0(Tesla T4) or 1(Tesla V100) or 2(A100)): \")\n",
    "            if tfgen(gputype) == \"True\":\n",
    "                gputype = \"NVIDIA Tesla V100\"\n",
    "            elif tfgen(gputype) == \"False\":\n",
    "                gputype = \"NVIDIA Tesla T4\"\n",
    "            elif tfgen(gputype) == \"2\":\n",
    "                gputype = \"NVIDIA A100 Tensor\"\n",
    "            else:\n",
    "                gputype = input(\"GPU Type (e.g: NVIDIA Tesla T4): \")\n",
    "                originalgpu = True\n",
    "            if originalgpu == False:\n",
    "                v_ram = str(input(\"VRAM容量(GB) (e.g: 16): \"))\n",
    "                v_ram = f\"{v_ram}GB\"\n",
    "                ram = str(input(\"RAM容量(GB) (e.g: 12): \"))\n",
    "                ram = f\"{ram}GB\"\n",
    "                memtype = input(\"Colab Runtime Memory Type (0(Normal Memory) or 1(High Memory)): \")\n",
    "                if tfgen(memtype) == \"True\":\n",
    "                    memtype = f\"High Memory (RAM: {ram} / VRAM: {v_ram})\"\n",
    "                elif tfgen(memtype) == \"False\":\n",
    "                    memtype = f\"Normal Memory (RAM: {ram} / VRAM: {v_ram})\"\n",
    "                else:\n",
    "                    memtype = f\"(RAM: {ram} / VRAM: {v_ram})\"\n",
    "                gpu = f\"{gputype}  {memtype}\"\n",
    "            else:\n",
    "                memtype = input(\"メモリタイプを入力 (e.g: (RAM: 32GB / VRAM: 24GB)): \")\n",
    "                gpu = f\"{gputype} {memtype}\"\n",
    "        envinfoprintout = f\"\"\"\n",
    "        # Enviroment Info\n",
    "            Machine Type: {machinetype}\n",
    "            GPU Type: {gpu}\n",
    "            \n",
    "        \"\"\"\n",
    "        b = True\n",
    "        break\n",
    "    elif next == \"No\":\n",
    "        print(training_printout + \"\\n\" + resulttrain_printout)\n",
    "        b = True\n",
    "        break\n",
    "    else:\n",
    "        b = False\n",
    "\n",
    "while c:\n",
    "    next = input(\"Are you want Result (Infer) Input? (Yes or No): \")\n",
    "    if next == \"Yes\":\n",
    "        part = str(input(\"Foldor Part Name (e.g: 2 = ./train_ichika/p2/*.wav): \"))\n",
    "        result_filename = input(\"Result File Name (e.g: result_p2.wav): \")\n",
    "        source = input(\"ソースタイトル: \")\n",
    "        speaker_id = str(input(\"Speaker ID: \"))\n",
    "        source_name = input(\"Source Audio: \")\n",
    "        transpose = str(input(\"Transpose: \"))\n",
    "        pitchalg_ = input(\"Pitch Extraction Algorithm: \")\n",
    "        embmodel = input(\"Embedder Model(0(hubertjp) or 1(contentvec)): \")\n",
    "        if tfgen(embmodel) == \"True\":\n",
    "            embmodel = \"ContentVec\"\n",
    "        elif tfgen(embmodel) == \"False\":\n",
    "            embmodel = \"Hubert-base-Japanese\"\n",
    "        else:\n",
    "            embmodel = tfgen(embmodel)\n",
    "        emboutlayer_ = str(input(\"Embedder Output Layer: \"))\n",
    "        autoli = input(\"Auto Load Index (0 or 1): \")\n",
    "        f0cf = input(\"F0 Curve File(No Input = Not Selected): \")\n",
    "        if f0cf == \"\":\n",
    "            f0cf = \"Not Selected\"\n",
    "        result = str(input(\"Result Count(現在 1 のみサポート): \"))\n",
    "        VocalPt = input(\"Vocal Point: \")\n",
    "        pronun = input(\"Pronunciation Point: \")\n",
    "        accuracy = input(\"Accuracy Point: \")\n",
    "        total = VocalPt + pronun + accuracy\n",
    "        total = str(total)\n",
    "        accuracy = str(accuracy)\n",
    "        pronun = str(pronun)\n",
    "        VocalPt = str(VocalPt)\n",
    "        resultinfer_printout1 = f\"\"\"\n",
    "        # Result (Infer)  - Source {source} -\n",
    "            Speaker ID: {speaker_id}  |  Source Audio: {source_name}\n",
    "            Transpose: {transpose}  |  Pitch Extraction Algorithm: {pitchalg_}\n",
    "            Embedder Model: {embmodel}\n",
    "            Embedder Output Layer: {emboutlayer_}\n",
    "            Auto Load Index: {autoli}\n",
    "            F0 Curve File: {f0cf}\n",
    "            Result Count: {result}\n",
    "        \n",
    "        # 結果\n",
    "        | Number | Source Name | Result File | Vocal | pronunciation | Accuracy | Total |\n",
    "        |---|---|---|---|---|---|---|\n",
    "        | No.1 | {source_name} | [Result 1 ~{source_name}~](./train_ichika/p{part}/{result_filename}) | {VocalPt} | {pronun} | {accuracy} | {total} |\n",
    "        \n",
    "        \n",
    "        # 点数の詳細\n",
    "            # No.1 ~{source_name}~\n",
    "                Vocal Point: \n",
    "                ({VocalPt} / 100)\n",
    "\n",
    "                pronunciation Point:\n",
    "                ({pronun} / 100)\n",
    "                \n",
    "                Accuracy Point:\n",
    "                ({accuracy} / 100)\n",
    "                \n",
    "                Total Point:\n",
    "                ({total} / 300)\n",
    "                \n",
    "        \"\"\"\n",
    "        c = True\n",
    "        break\n",
    "    elif next == \"No\":\n",
    "        print(training_printout + \"\\n\" + resulttrain_printout + \"\\n\" + envinfoprintout)\n",
    "        c = True\n",
    "        break\n",
    "    else:\n",
    "        c = False\n",
    "    \n",
    "\n",
    "print(training_printout + resulttrain_printout + envinfoprintout + resultinfer_printout)\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
